{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements:\n",
    "- 1. Add an .env file containing the OPENAI_API_KEY variable with a valid key\n",
    "- 2. Add knowledge_base.csv dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai faiss-cpu numpy pandas python-dotenv langchain-openai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 14:02:39,379 [INFO] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-24 14:02:40,066 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Warren McCulloch and Walter Pitts invented neural networks.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import openai\n",
    "import logging\n",
    "from typing import List, Any\n",
    "from dotenv import load_dotenv\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_env_variable(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves an environment variable.\n",
    "    Raises an error if the variable is not set.\n",
    "    \"\"\"\n",
    "    value = os.getenv(name)\n",
    "    if value is None or value.strip() == \"\":\n",
    "        raise EnvironmentError(f\"Required environment variable '{name}' is not set.\")\n",
    "    return value\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    Configuration settings for the RAG pipeline.\n",
    "    \"\"\"\n",
    "    OPENAI_API_KEY: str = field(default_factory=lambda: get_env_variable(\"OPENAI_API_KEY\"))\n",
    "    KNOWLEDGE_BASE_PATH: str = \"../data/knowledge_base.csv\"\n",
    "    FAISS_INDEX_PATH: str = \"../data/faiss_index.idx\"\n",
    "    EMBEDDING_MODEL: str = \"text-embedding-ada-002\"\n",
    "    CHAT_MODEL: str = \"gpt-3.5-turbo\"\n",
    "    TEMPERATURE: float = 0.1\n",
    "\n",
    "CONFIG = Config()\n",
    "openai.api_key = CONFIG.OPENAI_API_KEY\n",
    "\n",
    "class EmbeddingService:\n",
    "    \"\"\"\n",
    "    Service for generating text embeddings using OpenAI's API.\n",
    "    \"\"\"\n",
    "    def __init__(self, model: str = CONFIG.EMBEDDING_MODEL) -> None:\n",
    "        self.model = model\n",
    "    \n",
    "    def get_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generates an embedding for the given text.\n",
    "        \"\"\"\n",
    "        response = openai.embeddings.create(model=self.model, input=[text])\n",
    "        embedding = response.data[0].embedding\n",
    "        return np.array(embedding, dtype=np.float32)\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"\n",
    "    Handles loading and saving knowledge base data.\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_path: str = CONFIG.KNOWLEDGE_BASE_PATH) -> None:\n",
    "        self.csv_path = csv_path\n",
    "    \n",
    "    def load(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Loads the knowledge base from a CSV file.\n",
    "        \"\"\"\n",
    "        return pd.read_csv(self.csv_path)\n",
    "    \n",
    "    def save(self, df: pd.DataFrame, path: str) -> None:\n",
    "        \"\"\"\n",
    "        Saves the DataFrame to a specified CSV file.\n",
    "        \"\"\"\n",
    "        df.to_csv(path, index=False)\n",
    "\n",
    "class FaissIndexer:\n",
    "    \"\"\"\n",
    "    Builds and manages a FAISS index for fast text retrieval.\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame) -> None:\n",
    "        self.df = df\n",
    "        self.embedding_service = EmbeddingService()\n",
    "    \n",
    "    def build_index(self) -> faiss.IndexFlatIP:\n",
    "        \"\"\"\n",
    "        Constructs a FAISS index from text embeddings.\n",
    "        \"\"\"\n",
    "        embeddings = [self.embedding_service.get_embedding(row[\"Text\"]) for _, row in self.df.iterrows()]\n",
    "        vectors = np.array(embeddings)\n",
    "        norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "        vectors = vectors / norms\n",
    "        index = faiss.IndexFlatIP(vectors.shape[1])\n",
    "        index.add(vectors)\n",
    "        faiss.write_index(index, CONFIG.FAISS_INDEX_PATH)\n",
    "        return index\n",
    "    \n",
    "    def load_index(self) -> faiss.IndexFlatIP:\n",
    "        \"\"\"\n",
    "        Loads a FAISS index from file.\n",
    "        \"\"\"\n",
    "        return faiss.read_index(CONFIG.FAISS_INDEX_PATH)\n",
    "\n",
    "class Retriever:\n",
    "    \"\"\"\n",
    "    Retrieves the most relevant texts from the FAISS index.\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, index: faiss.IndexFlatIP) -> None:\n",
    "        self.df = df\n",
    "        self.index = index\n",
    "        self.embedding_service = EmbeddingService()\n",
    "    \n",
    "    def search(self, query: str, top_k: int = 3) -> List[str]:\n",
    "        \"\"\"\n",
    "        Searches the FAISS index for relevant documents.\n",
    "        \"\"\"\n",
    "        query_vector = self.embedding_service.get_embedding(query)\n",
    "        query_vector = query_vector / np.linalg.norm(query_vector)\n",
    "        distances, indices = self.index.search(np.array([query_vector]), top_k)\n",
    "        return [self.df.iloc[idx][\"Text\"] for idx in indices[0] if idx < len(self.df)]\n",
    "\n",
    "class ResponseGenerator:\n",
    "    \"\"\"\n",
    "    Generates AI responses based on retrieved context.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = CONFIG.CHAT_MODEL, temperature: float = CONFIG.TEMPERATURE) -> None:\n",
    "        self.client = openai.Client()\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def generate(self, query: str, relevant_docs: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        Generates a response using OpenAI's chat completion API.\n",
    "        \"\"\"\n",
    "        context = \"\\n\".join(relevant_docs)\n",
    "        prompt = f\"Use the following context to answer the query:\\n\\nContext: {context}\\n\\nQuery: {query}\"\n",
    "        logger.debug(\"Generating response for query: %s\", query)\n",
    "        response = self.client.chat.completions.create(model=self.model_name, messages=[{\"role\": \"system\", \"content\": prompt}], temperature=self.temperature)\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "class RAGPipeline:\n",
    "    \"\"\"\n",
    "    Orchestrates the RAG pipeline for text retrieval and response generation.\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        self.data_loader = DataLoader()\n",
    "        self.df = self.data_loader.load()\n",
    "        self.indexer = FaissIndexer(self.df)\n",
    "        self.response_generator = ResponseGenerator()\n",
    "    \n",
    "    def build_or_load_index(self) -> None:\n",
    "        \"\"\"\n",
    "        Loads an existing FAISS index or builds a new one if not found.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.index = self.indexer.load_index()\n",
    "        except Exception:\n",
    "            self.index = self.indexer.build_index()\n",
    "    \n",
    "    def run(self, query: str) -> None:\n",
    "        \"\"\"\n",
    "        Executes the full RAG pipeline for a given query.\n",
    "        \"\"\"\n",
    "        self.build_or_load_index()\n",
    "        retriever = Retriever(self.df, self.index)\n",
    "        relevant_docs = retriever.search(query)\n",
    "        response = self.response_generator.generate(query, relevant_docs)\n",
    "        print(\"Response:\", response)\n",
    "\n",
    "pipeline = RAGPipeline()\n",
    "query = \"Who invented NNs?\"\n",
    "pipeline.run(query)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
